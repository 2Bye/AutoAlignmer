{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import soundfile as sf\n",
    "from panns_inference import AudioTagging, SoundEventDetection, labels\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from service_utils import FrameVAD, Append\n",
    "\n",
    "### Load VAD Model\n",
    "STEP = 0.100\n",
    "WINDOW_SIZE = 0.100\n",
    "CHANNELS = 1 \n",
    "RATE = 16000\n",
    "FRAME_LEN = STEP\n",
    "\n",
    "CHUNK_SIZE = int(STEP * RATE)\n",
    "\n",
    "vad = FrameVAD('checkpoints/naint_vad_BackSilenceSpeech.nemo',\n",
    "               sample_rate = RATE, \n",
    "               frame_len=FRAME_LEN, \n",
    "               frame_overlap=(WINDOW_SIZE - FRAME_LEN) / 2, \n",
    "               offset=0, device='cuda')\n",
    "\n",
    "### Load ASR Model\n",
    "ASR_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\n",
    "                                                              \"stt_en_conformer_ctc_large\", map_location='cuda')\n",
    "\n",
    "\n",
    "### Load AudioTag model\n",
    "at = AudioTagging(checkpoint_path='checkpoints/audio_scene_checkpoint.pth', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_wav(audio_frame, n_non_speech_frames, save_name_file, sample_rate):\n",
    "    global count\n",
    "    torch.cuda.empty_cache()\n",
    "    signal = np.frombuffer(audio_frame, dtype=np.int16)\n",
    "    result = vad.transcribe(signal)\n",
    "    speech = controller.buffer_for_speech(audio_frame)\n",
    "    \n",
    "    labels = controller.buffer_for_labels(result[1])\n",
    "    if (len(labels) >= n_non_speech_frames):\n",
    "        switch = 'move_window'\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        if (labels != 'speech').sum() == n_non_speech_frames:\n",
    "            switch = 'send'\n",
    "            \n",
    "            if controller.total_in_buffer == n_non_speech_frames:\n",
    "                switch = 'refresh'\n",
    "                \n",
    "            else:\n",
    "                duration = speech.size / sample_rate\n",
    "                if duration < 25 and duration > 1:\n",
    "                    sf.write(save_name_file + '.wav', speech, sample_rate)\n",
    "                    ASR_predict = ASR_model.transcribe([save_name_file + '.wav'])[0]\n",
    "\n",
    "                    au, sr = sf.read(save_name_file + '.wav')\n",
    "                    if ASR_predict != '' and get_audio_tag(au):\n",
    "                        print(f'Success cut - {save_name_file}, write file')\n",
    "                        with open(save_name_file + '.txt', \"w\") as text_file:\n",
    "                            text_file.write(ASR_predict)\n",
    "                    else:\n",
    "                        print(f'Bad audio file - {save_name_file}, remove file')\n",
    "                        os.remove(save_name_file + '.wav')\n",
    "                else:\n",
    "                    print(f'Bad duration - {save_name_file}, skip this cut')\n",
    "\n",
    "            count += 1\n",
    "            speech = controller.buffer_for_speech(switch = switch)\n",
    "\n",
    "        labels = controller.buffer_for_labels(switch = switch)\n",
    "\n",
    "def get_audio_tag(audio_array):\n",
    "    '''\n",
    "    This function check audio on music\n",
    "    '''\n",
    "    audio_my = audio_array[None, :]  # (batch_size, segment_samples)\n",
    "    (clipwise_output, embedding) = at.inference(audio_my)\n",
    "\n",
    "    idx = np.argsort(clipwise_output[0])[::-1][0:5]\n",
    "    idx_to_lb = {i : label for i, label in enumerate(labels)}\n",
    "    \n",
    "    rate_tag = {}\n",
    "    for i in range(len(idx)):\n",
    "        rate_tag[idx_to_lb[idx[i]]] = np.mean(clipwise_output[:, idx[i]])\n",
    "    \n",
    "    if ([*rate_tag][0].lower() == 'speech') and ([*rate_tag][1].lower().find('speech') != -1):\n",
    "        return True\n",
    "    else:\n",
    "        if [*rate_tag.values()][1] < 0.1:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = 'example.wav'\n",
    "au, sr = sf.read(wav_file, dtype='int16')\n",
    "\n",
    "step_len = CHUNK_SIZE\n",
    "for i in range(int(au.size / step_len) + n_non_speech_frames):\n",
    "    frame = au[(i * step_len) : (i+1)*step_len]\n",
    "    align_wav(frame, n_non_speech_frames, f'examples/example_{count}', sr)\n",
    "\n",
    "print('Finish cuting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
